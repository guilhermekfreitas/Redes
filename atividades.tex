\documentclass[12pt,draft]{report}
\usepackage[latin1]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[brazil]{babel}
\usepackage{url}
%\usepackage{graphicx}
%\usepackage{listings}
%\usepackage{verbatim}
%\usepackage{subfigure}
%\usepackage{multicol}
%\usepackage{framed}
%\usepackage[noend]{algorithmic}
%\usepackage{algorithm}
%\usepackage{listings} % Códigos
\usepackage{amssymb} % Simbolos Matematicos
\usepackage{amsfonts} % Simbolos Matematicos
\usepackage{amsmath}
%\usepackage{amsthm}
%\usepackage{txfonts}
%\usepackage{float}

\newcommand{\letra}[1]{\paragraph{#1.}}

\title{Questões Sugeridas \\ \small{(Relativas ao livro do Kurose $3^a$ edição \cite{kurose-3ed})} \\ MO601A - Teleprocessamento e Redes \\ Semestre 2010.1}
\author{Daniel Cason}


\begin{document}

\maketitle
\newpage

\chapter{Redes de computadores e a Internet}

Define a Internet a partir de duas abordagens: componentes de software e hardware e em termos de uma infra-estrutura de rede que provê serviços para aplicações distribuídas.

Os dispositivos são chamados de \textbf{hosts} ou \textbf{end systems}, que são conectados entre si através de links de comunicação e switches de pacotes.
Diferentes links possuem taxa de transmissão distintas, medidas em bits/s. Por eles passam pacotes, contendo dados e cabeçalhos com meta-dados.
Dentre os switches se destacam os routers (normalmente no core da rede) e os link-layer switches (normalmente usados em redes de acesso).
Ambos fazem o forward de pacotes para seus destinos. A sequência de links e packets switches atravessados por um pacote enviado é chamado de rota ou path.

Os \emph{end systems} acessam a internet através de \emph{Internet Service Providers}, que são, per si, uma rede de packets switches e links.
Eles provém acesso aos usuários através os mais diversos tipos de enlaces e taxas de transmissão. Ao mesmo tempo, eles são provedores de conteúdo disponibilizado na Internet.
Os ISPs são também interconectados, formando uma hierarquia de redes.

Os \emph{end systems} usam protocolos para controle da troca de dados, nos quais se destacam o IP e o TCP. O primeiro define o formato dos pacotes enviados e recebidos entre os routers e os end systems. O segundo dá suporte aos principais protocolos usados para troca de informações.

A Internet também pode ser descrita como uma infra estrutura que provê serviços para aplicações, ditas distribuídas, que executam nos end systems e não no core da internet ou nos packet switches.
Os end systems ligados à Internet oferecem uma \emph{Application Programming Interface} que especifica como os softwares rodando em outros end systems podem acessar as informações disponibilizadas.
Para que estas informações sejam trocadas, a Internet provê serviços que permitem a comunicação entre estes elementos de software.

\section{Borda da Internet}

Os end systems são normalmente chamados de \emph{hosts}, pois são os lugares onde aplicativos da Internet rodam.
Normalmente estes aplicativos são formados por dois tipos programas diferentes: \emph{clients} e \emph{servers}.
Os primeiros normalmente solicitam e recebem algum serviço dos servers.
Para tal eles se comunicam através de trocas de mensagens seguindo algum protocolo pré-estabelecido e conhecido por ambos.
Neste nível, os links e switches são abstraídos, tratados como ``caixa-pretas'' que fazem a troca de dados entre os dois lados.

Outra possibilidade de comunicação é o modelo \emph{Peer to peer} o P2P.
Nele todos os hots agem tanto como clientes como servidores.

\subsection{Redes de acesso}

Seriam as redes que conectam os end systems ao primeiro roteador (dito de borda) no trajeto entre eles e outros end systems em outras redes.

\paragraph{Dial-up} Muito empregada nos anos 90 para acesso residencial. Aproveita a infra-estrutura telefônica e os hosts fazem uma chamada tradicional para algum ISP. Um aparelho chamado modem converte o sinal digital vindo dos PCs para um sinal analógico que é enviado ao ISP, que o reconverte ao formato digital. Apresenta baixas taxas de transmissão e ocupam a linha telefônica.

\paragraph{DSL} É o modo de acesso residencial mais difundido atualmente. Também aproveita a infra-estrutura telefônica, fazendo passar pelos pares trançados da telefonia três ``canais'': upstream, downstream e voz. Nas residências tem-se um \emph{splitter} que separa o canal de voz dos de dados, que são enviados a um modem específico que faz a conversão digital-analógico. Na outra parte tem-se os \emph{DSLAN} que separam o sinal (de várias residências) telefônico dos sinais de dados, digitalizados e repassados à Internet. A taxa de transmissão é considerável e depende da distância da residência e as estações, pois o sinal se degrada com a distância, por conta da atenuação e da interferência. Normalmente a banda upstream é menor que a downstream, sendo a DSL chamada de assimétrica. Outra vantagem é o uso simultâneo do telefone e da Internet. Não há chamada para o ISP como no dial-up e a conexão é persistente.

\paragraph{Cabo} Neste modo se aproveita da infra-estrutura de TV a cabo. 
Nela tem-se os \emph{head end} que fazem broadcast os canais de televisão, que viajam via fibra ótica até junções setoriais (\emph{neighboorhood-level}).
De lá saem cabos coaxiais que alcançam as residências, o que faz com que a arquitetura seja chamada \emph{hybrid fibre coax}.
Nas residências tem-se os chamados modens a cabo que dividem o sinal em uma banda upstream e uma maior downstream. Um dado importante é que o meio de comunicação é compartilhado, ou seja, todas as mensagens enviadas pelo \emph{head end} são recebidas em todas as casas e todas mensagens enviadas por um modem ocupam o meio, sendo necessário um protocolo para coordenar os envios de dados.
Outra consequência disto é que a banda downstream é compartilhada por todos usuários, fazendo com que melhores taxas sejam obtidas com menos usuários empregando o serviço, enquanto se poucos usam o serviço, toda a banda upstream pode ser usada por um usuário.

\paragraph{Fibber-To-The-Home} Visto que as taxas de transmissão via fibra ótica são superiores às dos cabos metálicos, algumas empresas fornecem este serviço às residências. Este acesso pode ser dedicado, com uma fibra por residência. Outra forma é o acesso compartilhado, feito com duas arquitetura \emph{active optical networks (AONs)} e \emph{passive optical networks (PONs)}. Na primeira tem-se um compartilhamento de rede como é feito na Ethernet. 
Na segunda tem-se nas residências equipamentos chamados \emph{optical network termination (ONT)} que recebem os dados de uma fibra ótica que vai até uma estação regional que é o um \emph{optical splitter} que une os sinais e envia para um \emph{optical line terminator (OLT)} que faz a transformação para sinais elétricos. O OLT é normalmente associado um grupo de telecomunicações, que também provê telefone e canais de TV. Assim como na conexão a cabo, todos os pacoes enviados pelo OLT são replicados pelo splitter e recebidos por todos os ONTs.

\paragraph{Ethernet} É o modo mais comum de acesso em LANs, conectadas a um roteador de borda. A transmissão se dá por pares-trançados isolados de metal que possui uma tecnologia particular de controle de acesso ao meio (que é compartilhado). Esta arquitetura usa de switches locais que conectam os end systems (clientes ou servidores) entre si e com o roteador.

\paragraph{WiFi} É a tecnologia IEEE 802.11, usada para acesso à rede a partir de dispositivos sem fio. Em uma WLAN os end systems enviam/recebem mensagens de/para uma estação base conectada a redes cabeadas ou \emph{wired}. Estas estações podem ser conectadas a uma LAN, à Internet diretamente ou uma rede de acesso com outra tecnologia qualquer. A limitação é o alcance das antenas das estações bases, que se restringe a alguns metros. Há possibilidade de antenas mais potentes, para áreas maiores de abrangência.

\paragraph{Wide-Area Wireless Access} Outra possibilidade são as WLANs que permitem o acesso de dispositivos empregando a infra-estrutura existente para a telefonia celular, usando as antenas como estações bases, com alcance muito maior. Em desenvolvimento nos últimos tempos é a tecnologia 3G que provê acesso sem fio de largo alcance, via comutação de pacotes e com velocidades mais altas.

\paragraph{WiMAX} A IEEE 802.16 é uma concorrente para a WiFi. Eles prometem altas velocidades e operariam independentemente à infra-estrutura da telefonia celular.

\subsection{Meios de transmissão}

Esta seção analiza os meios usados para transmissão dos bits nas redes. Há a distinção de meios guiados e meio não guiados (tecnologia ``sem fio'').

\paragraph{Duplo par-trançado}
Tecnologia mais barata, consiste em um par de fios trançados (para reduzir interferências) e isolados (com não condutores) e está presente na maior parte das residências, por ter ser usado em cabeamento telefônico. Estes cabos são usados no acesso dial-up (<56 kbps) e DSL (passam de 6Mbps). Os \emph{Unshielded Twister Pair (UTP)} são os mais usados em LANs de alta velocidade, tendo velocidades entre 10 Mbps e 1Gbps. Eles consistem em vários pares-trançados embalados em material isolante, para evitar interferência magnética.

\paragraph{Cabo coaxial}
Consiste em dois condutores concêntricos isolados entre si e isolados do meio. É mais rígido e tem altas taxas de transmissão, além de poder ser usado como meio compartilhado, podendo vários computadores se conectar a um cabo e ter acesso a todo conteúdo trocado.

\paragraph{Fibra ótica}
Os bits são modulados em comprimentos de luz que caminha em um cabo isolado, flexível e fino, se refletindo em suas paredes.
Tem taxas de transmissão muito alta, pouca interferência e atenuação, sendo usados para comunicação de longo alcance. 
Sua principal limitação é o preço. Suas velocidades nos modelos $OC-n$ são da faixa de $51,8x Mbps$, havendo já tecnologias para $n$ 1,3,12,24,48,96, 192, 768.

\paragraph{Rádio - canais terrestres}
Tem a vantagem de não precisar de meio físico (é o ar), mas sofre de (auto)-interferência, atenuação que limitam sua abrangência. 
Nas LANs usa-se frequências específicas, com alcance limitado. Nas redes celulares, tem-se tecnologias de maior alcance e menores taxas.

\paragraph{Rádio - satélites}
Neste modo satélites posicionados no espaço recebem transmissões em uma frequência e retornam em outra frequência. Há dois tipos principais: os geoestacionários, que não se ``movem'' em relação ao solo (são poucos) e os de baixa órbita, que giram com a terra e tem que se comunicar entre si (tem menor alcance), podendo ser usados no futuro.
As taxas de transmissão são altas, mas tem-se um atraso considerável (principalmente nos geoestacionário), chegando a 4 segundos.

\section{The Internet core}

Inicialmente define-se os modelos de comunicação, ou seja, de transporte das mensagens entre os end systems através dos links e switches.

\subsection{Comutação por circuitos}

Nesta forma de organização tem-se que o caminho entre dois end systems deve ser reservado antes do início de uma troca de dados.
Assim, antes da troca de dados é criado um \emph{circuito} entre os transmissores, reserva-se uma parcela da capacidade de transmissão para esta \emph{conexão}.
Os dados, então, trafegam na rede por um caminho pré-determinado, com garantia de uma dada taxa de transmissão e um limitante para os atrasos.

Neste modelo, um link é compartilhado por uma série de conexões, que obtém parte de sua capacidade.
Esta divisão pode ser realizada de duas formas.

A primeira é a \emph{Frequency-division multiplexing (FDM)}, onde a capacidade de transmissão do meio é dividida em faixas de frequências distintas e isoladas que serão usadas por conexões diferentes, tal como as frequências de rádio são distribuídas entre diferentes emissoras.
Obviamente, a taxa de transmissão é tão grande quanto maior for a amplitude da faixa (ou largura de banda) reservada.

A segunda forma é \emph{Time-division multiplexing (TDM)}, onde a banda de transmissão é dividida slots de tempo, reservados às conexões.
Assim, cada conexão usa o canal para suas transmissões por um slot de tempo, resultando que a taxa obtida por ela é a taxa do canal divida pela quantidade de slots existentes.

Este modelo é o mais empregado (apesar de em migração) pelas redes telefônicas, visto as suas garantias de taxas de transmissão e de atraso.
Vale notar que em tais transações, a quantidade de dados trocados costuma ser constante e tem-se exigências de qualidade, pois são empregadas para interações em tempo real.

As limitações do modelo são claras: o subuso dos links e as limitações em escalabilidade.
Como cada conexão reserva para si uma fatia da capacidade de transmissão, o número de clientes por canal é claramente limitado e de remanejamento não trivial.
Além disto, quando não há efetiva troca de dados os slots de tempo ou faixas de frequências não são usados e outras conexões não podem se apossar de tal capacidade inutilizada.

\subsection{Comutação por pacotes}

Ao contrário do modelo anterior, aqui não se reservam recursos e nenhuma rota (ou circuito) é estabelecida com antecedência.
Os dados são quebrados em várias partes de tamanho limitado, chamados \emph{pacotes}. 
Estes pacotes são lançados na rede com destino a um roteador mais próximo, a partir do qual são repassados a outros roteadores e links até o destino.

Cada pacote ocupa o canal por inteiro, empregando toda sua capacidade para transmitir os bits, que vão sendo armazenados no \emph{packet switch} até que todo o pacote seja entregue.
Finalizada a recepção, o switch encaminha o pacote para o link de saída. 
Caso ele já esteja ocupado por algum pacote, o pacote aguardará em uma fila até que chegue sua vez de ser transmitida.
Como estas filas são \emph{buffers} nos switches, elas são finitas e caso já estejam repletas, o pacote será descartado.

Nota-se aí grandes diferenças do modelo anterior, geradas pelo fato de que nenhuma banda foi reservada.
Tem-se um primeiro atraso que depende do tamanho do caminho percorrido pelo pacote. Tendo ele $L$ bits, passando por $Q$ switches conectados por canais de taxa de transmissão $R$, tem-se um atraso decorrente deste método de \emph{store-and-forward} de $QL/R$.
O segundo atraso é o de enfileiramento, ou seja, o tempo que o pacote aguarda na fila de saída antes de usar os links.
Este é normalmente imprevisível e depende do qual sobrecarregada esteja a rede, fator que tende a aumentar muito os atrasos e até a levar à \emph{perdas} de pacotes, que não cabem na fila.

Apesar destes problemas, este é o modelo empregado prioritariamente na Internet.
Ele é baseado no conceito de \emph{best effort}: os pacotes são transmitidos da melhor forma possível, dadas as circunstâncias.
Ou seja, toda a banda disponível é alocada para os pacotes em tráfego e pouca ou nenhuma capacidade de transmissão é disperdiçada.
Por outro lado, permite-se que a rede seja sobrecarregada, levando a grandes atrasos de enfileiramento, possível congestionamento da rede e perdas.

Outra diferença importante é que em circuitos o pacote é enviado a uma mesma taxa durante todo o caminho percorrido.
Esta taxa é limitada, porém garantida.
Usando-se pacotes, a taxa efetiva vai ser a menor taxa dentre as disponíveis no caminho e tem-se todos os atrasos por enfileiramento, resultantes da sobrecarga dos canais de saída.
Nota-se que esta sobrecarga pode ocorrer simplesmente pelo fato de um link de entrada ter taxa superior a um link de saída.

Os recursos são alocados sob demanda, o que leva a se considerar que neste modelo se realiza \emph{multiplexação estatística}.
Se há poucos usuários ou se as conexões não são persistentes (o que é bem realista), consegue-se taxas melhores que no modelo de circuitos e também que mais conexões compartilhem um mesmo link.

\subsection{ISPs e Backbones da Internet}

Há uma hierarquia de ISPs na Internet, que são conectados um com os outros a fim de fazer com que as mensagens possam ser trocadas por diferentes pares de end systems pelo mundo.

A parte mais alta desta hierarquia são os ISPs \emph{tier-1}, chamados de \emph{Internet backbone}.
Eles são dotados de links de alto desempenho e roteadores robustos e são conectados entre si (são \emph{peers}) e por eles passa boa parte do tráfego da Internet.

Abaixo deles há os ISPs \emph{tier-2}, que se conectam a um ou mais ISPs \emph{tier-1}, possivelmente com outros \emph{tier-2} e, finalmente, com outros níveis inferiores da hierarquia.
No nível mais baixo desta hierarquia estão as diversas redes de acesso, apesar de algumas instituições terem várias conexões distintas, se lingando muitas vezes diretamente aos \emph{tiers} mais altos.
Em particular, \emph{tiers} mais altos são ditos provedores dos \emph{tiers} mais baixos, que são seus usuários.

Os pontos de interconexão entre ISPs são chamados de \emph{Pontos de Presença (POP)} e contém uma série de roteadores que fazem comunicação entre estas grandes redes.
Cada ISP de \emph{tier-1} tem uma série de POPs espalhados geograficamente pelo mundo, que os conectam com os \emph{tiers-2} regionais ou com outros \emph{tiers-1}.

\section{Atrasos, perdas e throupughput em redes comutadas por pacotes}

\subsection{Atrasos}

Supondo que um host envia um pacote para outro host, este pacote passa por uma série de links e de nós da rede.
A cada nó há uma série de atrasos associados a cada pacote que chega por um enlace.

O primeiro atraso é o de \emph{processamento}, onde computa-se o tempo de checar a integridade do pacote (se o protocolo de enlace requerer), extrair o cabeçalho e verificar em uma tabela por qual link aquele pacote deve sair, o que dura alguns microsegundos ou até menos.

O segundo atraso é o de \emph{enfileiramento} que ocorre quando há outros pacotes sendo enviados e/ou já enfileirados no enlace de saída. 
Ele é variável e depende do tráfego da rede, sendo da ordem de microsegundos até milisegundos.

Como os roteadores usam a técnica de store-and-forward, há uma atraso de \emph{transmissão} da ordem de $L/R$: tamanho do pacote/taxa do link de saída.
Há também o tempo de \emph{propagação} de cada bit que é dado pela distância entre os dois roteadores sobre velocidade das ondas no meio.
Esta varia entre $2$ e $3 . 10^8 m/s$, que é a velocidade da luz no vácuo.

Assim, o \emph{atraso nodal} (relativo a cada nó) é dado por $d_{nodal} = d_{proc} + d_{queue} + d_{trans} + d_{prop}$.
Em casos distintos, tem-se diferentes atrasos predominantes: rede congestionada seria o $d_{queue}$, transmissões via satélite o $d_{prop}$, baixa taxa de transmissão aumenta o $d_{trans}$ e o $d_{proc}$ é mais relevante para computar a capacidade total de processamento de dados de um roteador.

Enfim, no atraso fim-a-fim são computados os atrasos de cada nó (roteador) da rede $d_{proc} + d_{queue}$ 
e o atraso de cada link da rede $d_{trans} + d_{prop}$.

\subsection{Atraso de enfileiramento e perdas}

Este é o atraso mais importante, normalmente, pois ele é variável: diferentes pacotes numa mesma rota podem ter atrasos deste tipo distintos.
Como a chegada de pacotes normalmente ocorre com uma frequência variável, estas métricas são estatísticas.
Seja $a$ a taxa média de pacotes que chegam por segundo, $La$ é o número de bits que chegam e a razão $La/R$ é importante.
Se esta fração se aproxima de um, a fila cresce indefinidamente, e nas proximidades o crescimento da fila é exponencial com o aumento de $a$.
Como os buffers não são infinitos, as filas não crescem indefinidamente e ocorrem perdas de pacotes.

\subsection{Throughput}

Contabiliza a taxa em bits/segundo em que os dados são transferidos entre emitente e receptor.
Como este valor tende a variar, trabalha-se com o \emph{throupughput médio} e o \emph{throupughput instantâneo} que apresentam valores interessantes para transmissões longas e para para interesses imediatos.

O throupughput entre dois hosts, supondo ausência de outro tráfego, será igual à menor taxa de transmissão dentre os links intermediários.
Este link é chamado \emph{bootleneck link}.
Na prática, os links do core da Internet tem taxas muito elevadas e assim a limitação está nos links das bordas da rede, ou seja, às taxas fornecidas aos hosts nas redes de acesso.

Outra possibilidade é que várias transferências estejam sendo feitas de vários servidores em uma rede de acesso para outros tantos clientes em outra rede de acesso.
Caso haja, entre eles, um canal com capacidade inferior à soma das taxas de cada hosts na rede de acesso, ele pode vir a ser um bootleneck.

É importante ressaltar que \emph{throupughput} e \emph{atraso} são medidas distintas, sendo mais ou menos interessantes para cada aplicação específica.
Assim, transmissão de voz ou vídeo precisam de ambos, enquanto para uma transferência de um arquivo grande, um throupughput médio maior seria o mais interessante.

\section{Camadas}

Discute-se a definição de camadas, de uma pilha de camadas com seus protocolos para estudo e implementação de redes.
Em prática, dentro de um mesmo host cada camada recebe solicitações da superior e usa serviços da inferior.
Entre dois hosts, as camadas equivalentes se comunicam através de protocolos.

Emprega-se a pilha protocolos TCP, como se segue.
A camada mais alta é a de \emph{aplicação}, que troca \emph{mensagens}.
Abaixo tem-se a camada de transporte, que usa o formato de \emph{segmentos}; 
a camada de rede, que usa \emph{datagramas};
a de enlace que usa \emph{frames} e a física, responsável pela troca efetiva de dados e específica para cada tecnologia de comunicação empregada.

Cada um destes ``pacotes'' de cada camada contém os dados (que originalmente vem da aplicação) e um cabeçalho específico, 
com informações que são úteis à camada equivalente no host que receber tal ``pacote'' na mesma camada.
Em particular, em aplicação e transporte a comunicação se dá entre os end systems.
Já nas demais, a comunicação é entre cada par de nós no caminho fim-a-fim.

\section{Segurança em Redes}

Esta seção fala por cima de alguns problemas de segurança principais em redes de computadores.
Dentre eles tem-se: 
\paragraph{IP spoofing} falseamento de endereço e recepção de conteúdo alheio
\paragraph{Man-in-the-midle} interceptação de comunicação fim-a-fim, através de infiltração no core da rede
\paragraph{Packet sniffing} possibilidade de ver tráfego alheio em redes compartilhadas
\paragraph{Denial-of-service} faz com que um servidor fique inutilizável, através de invasões, sobrecarga de solicitações ou de conexões (usando o fato do TCP usar conexão em três vias)
\paragraph{Ataque a hosts com malware} São os trojans (execução de algum programa ou script infectado), worms (infecção sem intervenção do usuário) e Trojan horses (aplicativos mal intencionados mascarados como aplicativos normais). Os dois últimos costumam usar o hosts para se replicar junto aos contatos do usuário.

\section{História da Internet}

Separada em fases:
\paragraph{61-72} são criadas as bases para a comutação de pacotes. Culmina na ARPAnet de 72.
\paragraph{72-80} ligação entre redes e novas redes proprietárias ALOHA, Ethernet, DECnet, arquitetura de Cerf e Kahn, princípios da ATM
\paragraph{80-90} TCP/IP (83), smtp, dns, ftp, controle de congestionamento TCP. Redes nacionais e cerca de 100000 hosts conectados.
\paragraph{90-2000} hypertext, html, Netscape e comercialização de produtos para Web. P2P, backbones em Gbps, 50 milhões de hosts e cerca de 100 milhões de usuários
\paragraph{atual} cerca de 500 milhões de hosts (2007), Voip, P2P, wireless, mobilidade



%%


\section{Questões}

\subsection{Q4}
Quais são os dois tipos de serviços de transporte que a Internet provê às suas aplicações? Cite algumas características de cada um destes serviços

%TODO

\subsection{Q5}
Afirma-se que controle de fluxo e controle de congestionamento são equivalentes. Isso é válido para o serviço orientado para conexão da Internet? Os objetivos do controle de fluxo e do controle de congestionamento são os mesmos?

%TODO

\paragraph{Q7}
Qual é a vantagem de uma rede de comutação de circuitos em relação a uma de comutação de pacotes? Quais são as vantagens da TDM sobre a FDM em uma rede de comutação de circuitos?

As vantagens das redes comutadas por circuito são as garantias dadas à transmissão de dados. 
Como antes da transmissão são alocados recursos nos nós intermediários, tem-se uma taxa garantida de transmissão e atrasos controlados.
Assim, a priori, toda a transmissão se dá sem jitter, com atraso constante e a taxas pré-estabelecidas.

Tem vantagem?
%TODO


\paragraph{Q9}
Suponha que exista exatamente um comutador de pacotes entre um computador de origem e um de destino. As taxas de transmissão entre a máquina de origem e a máquina de destino são $R_1$ e $R_2$, respectivamente. Admitindo que um roteador use comutação de apcotes do tipo armazena-e-reenvia, qual é o atraso total fim-a-fim para enviar um pacote de comprimento $L$? (Desconsidere a formação de fila, atraso de propagação e atraso de processamento.)

O pacote é enviado a uma taxa $R_1$ pelo primeiro enlace. Ele então, é armazenado no comutador, o que leva $L/R_1$ segundos.
Passado este tempo, o pacote é enviado pelo segundo link, chegando por completo ao destino em $L/R_2$ segundos.
Assim, o atraso total é de $L(1/R_1 + 1/R_2)$.

\paragraph{Q11}
Suponha que você esteja desenvolvendo o padrão para um novo tipo de rede de comutação de pacotes e precisa decidir se seu rede usará CVs ou roteamento de datagramas. Quais são os prós e os contras da utilização de CVs?

No roteamento por datagramas, cada pacote deve conter o endereço de destino, que sendo único, é mais extenso que somente o CV, que é o identificador da conexão.
Assim o tempo de processamento a cada nó tende a ser menor empregando-se CVs, assim como o tamanho dos cabeçalhos dos pacotes.

Por outro lado, antes de enviar o pacote será necessária uma operação anterior que faça a descoberta da rota entre emitente e destinatário e que preencha as tabelas de cada comutador intermediário.
Neste caso, estes sinais para tal transação terão que conter, de alguma forma, o endereço único do emitente e receptor.
Assim, mesmo empregando CVs haverá um processamento inicial que depende de informações de roteamento.
Se a transmissão for restrita a poucos pacotes, este esforço e atraso iniciais podem passar a ser relevantes, tornando esta abordagem desvantajosa.

\paragraph{Q12}
Cite seis tecnologias de acesso. Classifique cada uma delas nas categorias acesso residencial, acesso coorporativo ou acesso móvel.

Há duas tecnologias que empregam a infra-estrutura telefônica existente, sendo prioritariamente empregadas em acesso residencial: a DSL e a dial-up.
Outra opção para residências é a internet a cabo, que aproveita a infra-estrutura existente para as televisões por assinatura.

O acesso corporativo, pode empregar estes métodos também, mas o custo/benefício seria questionável.
Normalmente eles preferirão links dedicados de comunicação.
Apesar de poder ser usada também para acesso residencial, redes de acesso empregando fibras ópticas são uma alternativa para coorporações.

As principais tecnologias de acesso móvel são as WLANs e as redes celulares.
As primeiras são redes sem fio de curto alcance, contendo uma antena fixa que se conecta à rede cabeda, que normalmente emprega outra tecnologia.
As redes celulares empregam a infra-estrutura para telefonia móvel, ou seja, as antenas que dão cobertura à células de grande extensão.
O mesmo sinal empregado para a comunicação e troca de mensagens de texto pode ser empregada para transporte de dados, ou seja, para acesso à Web.
Apesar de oferecer taxas ainda reduzidas de transmissão, a tecnologia 3G e possíveis novidades tendem a aumentar a largura de banda desta opção.

Dentro de coorporações a tecnlogia predominante é a Ethernet. Ela é uma arquitetura escalável e que permite um acesso em altas velocidades a recursos locais (dentro da mesma rede) e a roteadores que comunicam a rede com o restante da Internet.


\paragraph{Q18}
Modens discados, HFC e ADSL são usados para acesso residencial. Para cada uma dessas tecnologias de acesso, cite uma faixa de taxas de transmissão e comente se a largura de banda é compartilhada ou dedicada.

O acesso dial-up é o que oferece menor velocidade de acesso, sendo limitado a 56kbps, que á taxa máxima que pode ser transferia nos 4Khz de largura de banda da conexão telefônica (o mais que suficiente para voz, porém).
As conexões são dedicadas e empregam todo o canal, impedindo que o telefone seja empregado simultaneamente.

A ADSL emprega também as linhas telefônicas, mas ao contrário da dial-up, emprega uma faixa de frequência maior, tendo canais para voz (os mesmos 4Khz para o telefone), upstream e downstream, sendo superior a taxa de downstream.
As limitações de velocidade da técnica estão diretamente à distância das casas para as centrais telefônicas regionais. 
Com esta tecnologia consegue-se velocidades entre algumas centenas de kbps até 3,6,8 Mbps na maioria das localidades e a largura de banda é dedicada (pois usa a mesma infra-estrutura telefônica, exclusiva para cada residência.

Vale lembrar que em ambos casos as várias conexões são multiplexadas nas centrais telefônicas, mas espera-se que a banda prometida seja mantida nesta operação.

O acesso HFC final é feito através de cabos coaxiais, que ligam várias casas à uma central regional. Pelas características da tecnologia, o canal é compartilhado, assim como a banda.
Chegando a tais estações, o tráfego é enviado via fibra ótica às centrais (ou a outras regionais de hierarquia superior), realizando a multiplexação dos sinais dos diversos cabos. Novamente, espera-se que esta operação não cause perdas de velocidade.
OS acessos nesta mobilidade oferecem atualmente taxas bem variadas, desde algumas centenas de kbps até 6, 8 ou 10 Mbps.

\paragraph{Q19}
Considere o envio de um pacote de uma máquina de origem a uma de destino por uma rota fixa. Relacione os componentes do atraso que formam o atraso fim-a-fim. Quais deles são constantes e quais deles são variáveis?

A rota é formada por uma série de nós intermediários e associa-se o atraso a cada nó. O primeiro atraso é associado à tecnologia store-and-forward empregada. Ele é inversamente proporcional a taxa de transmissão do link, que no exemplo será constante.
O segundo atraso tem é o processamento dos nós, que inclui a verificação da consistência dos pacotes, a extração do cabeçalho e a computação do link de saída a ser empregado. Este atraso é constante e nele costuma pesar mais o cálculo do link de saída.
Outro atraso é o de propagação, que depende da velocidade da luz no meio empregado para transmissão, ou seja, dos links, sendo constante no exemplo.

O único atraso variável é o de enfileiramento. 
Este é o tempo que o pacote permanece retido nos nós aguardando a liberação do link de saída que deve ser empregado. Este tempo é o restante para que o envio do pacote que ocupa o link seja encerrado mais o tempo empregado par enviar todos os pacotes que se encontrem enfileirados assim como o em questão e que tenham maior prioridade de uso do link (normalmente por terem chegando antes).

\paragraph{Q21}
Quais são as cinco camadas da pilha do protocolo da Internet? Quais as principais responsabilidades de cada uma dessas camadas?

A camada superior é a de aplicação. Ela contém protocolos padrões da Internet, empregados pelas aplicações tanto para interagir com os serviços disponibilizados na Internet (como cliente) como para serví-los (como servidor).

Tem-se depois a camada de transporte, que presta serviço à de aplicação, abstraindo fatores associados à transmissão de dados.
Ela contém prioritariamente dois protocolos, o TCP e o UDP. Assim como estes, os protocolos desta camada diferem nas garantias que são dadas à camada superior quanto a transmissão de dados.
Dentre elas tem-se a entrega garantida, entrega em ordem, consistência dos dados.
Além destas garantias, a camada de transporte é responsável por permitir que várias aplicações possam usar simultaneamente a rede, como se a estivessem usando exclusivamente.

A próxima camada é a de rede, que é responsável por abstrair, para a camada de transporte todos os passos intermediários que um pacote deve percorrer antes de chegar ao destino.
Enquanto as camadas acima lidam com transmissões fim-a-fim entre hosts, esta camada é responsável por realizar o cálculos de rotas e realizar a comutação de pacotes tanto nos hosts como nos nós intermediários.
Na Internet este é o papel do protocolo IP, que associa um endereço único a cada host na Internet e permite que os pacotes sejam enviados pelos links corretos, sendo comutados para o próximo link na rota calculada a cada passo, chegando ao destino final.
Nota-se que nesta camada não tem-se a preocupação semântica sobre os dados: não sabe-se de que aplicação eles vem e não se preocupa-se com possíveis perdas, duplicatas, desordenações etc.

Abaixo dela tem-se a camada de enlace. Ela é responsável por realizar a transmissão de dados entre cada par de nós da rede, tendo protocolos específicos para uso do meio, detecção e possível correção de erros. Desta forma, ao contrário das outras, os protocolos desta camada são específicos para a(s) tecnologia(s) de rede empregadas por cada equipamento da rede.

Finalmente tem-se a camada física, que é responsável pelo efetivo envio de bits entre dois equipamentos (nós ou hosts) ligados por um link de conexão.
Assim como a de enlace, esta camada é implementada nos dispositivos de rede empregados, que se ligam diretamente ao link de transmissão.

\paragraph{Questão 23}
Que camadas da pilha do protocolo da Internet um roteador implementa? Que camadas um comutador de camada de enlace implementa? Que camadas um sistema final implementa?

Um roteador implementa todas as camadas que não são fim-a-fim, ou seja, a de rede, enlace e física.

Um comutador de enlace não realiza comutação de pacotes nem cálculo de rotas, empregando apenas as camadas de enlace e rede.

Os hosts implementam todas as camadas.

\subsection{Problema 6}

Este problema elementar começa a explorar atrasos de propagação e de transmissão, dois conceitos centrais em redes de computadores. Considere dois computadores, A e B, conectados por um único enlace de taxa $R$ bps. Suponha que esses computadores estejam separados por $m$ metros e que a velocidade de propagação ao longo do enlace seja de $s$ metros/segundo. O computador A tem de enviar um pacote de $L$ bits ao computador B.

\paragraph{a.} Expresso o atraso de propagação $d_{prop}$ em termos de $m$ e $s$.

$d_{prop} = m/s$ segundos.

\paragraph{b.} Determine o tempo de transmissão do pacote $d_{trans}$, em termos de $L$ e $R$

$d_{trans} = L/R$ em segundos, pois $L$ é em bist e $R$ em bits/segundos.

\paragraph{c.} Ignorando os atrasos de processamento e de fila, obtenha uma expressão para o atraso fim-a-fim

$d = L/R + m/s$, ou seja, a soma dos dois anteriores.

\paragraph{d.} Suponha que o computador A comece a transmitir o pacote no instante $t = 0$. No instante $t = d_{trans}$, onde estará o último bit do pacote?

O primeiro bit do pacote chega em $d = d_{prop}$ a B. O último bit estará no link de transmissão se $d_{prop} * R \geq L$. Caso contrário, estará ainda em algum buffer do host A.

\paragraph{e.} Suponha que $d_{prop}$ seja maior que $d_{trans}$. Onde estará o primeiro bit do pacote no instante $t = d_{trans}$?

Atravessando o link.

\paragraph{f.} Suponha que $d_{prop}$ seja menor que $d_{trans}$. Onde estará o primeiro bit do pacote no instante $t = d_{trans}$?

No buffer do host B.

\paragraph{g.} Suponha $s = 2,5 . 10^8$, $L = 100$ bits e $R = 28$ kbps. Encontre a distância $m$ de forma que $d_{prop}$ seja igual a $d_{trans}$.

$m/s = L/R \implies m = s*L/R = 25.10^9 / 28.10^3 < 10^6 m$

\subsection{Problema 14}
Suponha que dois computadores, A e B, estejam separados a uma distância de 10 mil quilômetros e conectados por um enlace direto de $R = 1$ Mbps. Suponha que a velocidade de propagação pelo enlace seja de $2.5 . 10^8$ metros por segundo.

\paragraph{a.} Calcule o produto largura de banda-atraso $R . t_{prop}$

$10^6 . 10. 10^6 / 2.5 . 10^8 = 4 . 10^4$

\paragraph{b.} Considere o envio de um arquivo de 400 mil bits do computador A para o computador B. Suponha que o arquivo seja enviado continuamente, como se fosse uma única grande mensagem. Qual é o número máximo de bits que está no enlace a qualquer dado instante?

Seja um bit qualquer, ele demora $4 . 10^{-2}$ segundos para chegar a B. Neste tempo, a quantidade de bits que entram no link é proporcional a sua taxa de transmissão.
Assim, há $10^6 * 4 . 10^{-2}$ no canal, ou seja, $4 . 10^4$ bits, que é o produto banda-atraso.

\letra{c} Interprete o produto largura de banda-atraso.

Feito.

\letra{d} Qual é o comprimento (em metros) de um bit no enlace? É maior do que a de um campo de futebol?

O máximo de bits que podem estar simultaneamente no enlace é $4. 10^4$. Supondo ``comprimento uniforme'', ele será de $10 . 10^6 / 4.10^4 = 2,5.10^2 = 250m$. São mais de dois campos de futebol de $100m$.

\letra{e} Derive uma expressão geral para o comprimento de um bit em termos da velocidade de propagação $s$, da velocidade de transmissão $R$ e do comprimento do enlace $m$.

$c = m/(t_{prop} * R) = m(m/s * R) = s/R$.

\subsection{Problema 16}
Considere o problema 14, mas agora com um enlace de $R = 1$ Gbps.
\letra{a} Calcule o produto largura de banda-atraso, $R . t_{prop}$

$10^9 * 10.10^6 / 2,5.10^8 = 10^9 * 4.10^{-2} = 4.10^7$ bits.

\letra{b} Considere o envio de um arquivo de 400 mil bits no computador A para o computador B. Suponha que o arquivo seja enviado continuamente, como se fosse uma única grande mensagem. Qual será o número máximo de bits que estará no enlace a qualquer dado instante?

Como $4.10^7 > 400.10^3 = 4.10^5$, todo arquivo estará no enlace.

\letra{c} Qual é o comprimento (em metros) de um bit no enlace?

$ c= s/R = 2,5.10^8/10^9 = 2,5.10^{-1} = 25cm$

\subsection{Problema 17}
Novamente com referência ao problema 14.
\letra{a} Quanto tempo demora para enviar o arquivo, admitindo que ele seja enviado continuamente?

$L/R = 400.10^3/10^6 = 400.10^{-3} = 0,4 = t_{trans}$ e $t_{prop} = 10.10^6 / 2,5.10^8 = 4.10^{-2} = 0,04$.
O atraso será $t_{trans} + t_{prop} = 0,44$ segundos.

\letra{b} Suponha agora que o arquivo seja fragmentado em dez pacotes e que cada pacote contenha 40 mil bits. Suponha que cada pacote seja verificado pelo receptor e que o tempo de transmissão de uma verificação de pacote seja desprezível. Finalmente, admita que o emisso não possa enviar um pacote até que o anterior tenha sido reconhecido. Quanto tempo demorará para enviar o arquivo?

Assim serão 10 pacotes de 40 mil bits. Cada pacote tem atraso de $L/R + m/s = 40.10^3/10^6 + 10.10^6/2,5.10^8 = 40.10^{-3} + 4.10^{-2} = 80.10^{-3}$.
Além disto, cada confirmação demora $40.10^{-2}$ para chegar, levando a um atraso total de $10*(120.10^{-3}) = 1,2$ segundos.

\letra{c} Compare os resultados de `a' e `b'.

A transmissão por fluxo contínuo é muito mais rápida.

\subsection{Problema 20}
Em redes modernas de comutação de pacotes, a máquina de origem segmenta mensagens longas de camada de aplicação (por exemplo, uma imagem ou um arquivo de música) em pacotes menores e os envia pela rede. A máquina destinatária, então, monta novamente os pacotes restaurando a mensagem original. Denominamos esse processo \emph{segmentação de mensagem}. A Figura 1.21 ilustra o transporte fim-a-fim de uma mensagem e sem segmentação. Considere que uma mensagem de $7,5 . 10^6$ bits de comprimento tenha de ser enviada da origem ao destino da Figura 1.32. Suponha que a velocidade de cada enlace seja 1,5 Mbps. Ignore atrasos de propagação, de fila e de processamento.
\letra{a} Considero o envio da mensagem da origem ao destino \emph{sem} segmentação. Quanto tempo essa mensagem levará para ir da máquina de origem até o primeiro comutador de pacotes? Tendo em mente que cada comutador usa comutação de pacotes do tipo armazena-e-reenvia, qual é o tempo total para levar a mensagem da máquina de origem à máquina de destino?

O tempo até o primeiro comutador é $t = 7,5. 10^6 / 1,5 . 10^6 = 5s$. Para chegar ao destino, o tempo é o triplo pois deve-se passar por outro comutador e chegar ao destino final.

\letra{b} Agora suponha que a mensagem seja segmentada em 5 mil pacotes, cada um com 1.500 bits de comprimento. Quanto tempo demorará para o primeiro pacote ir da máquina de origem até o primeiro comutador? Quando o primeiro pacote está sendo enviado do primeiro ao segundo comutador, o segundo pacote está sendo enviado da máquian de origem ao primeiro comutador. Em que instante o segundo pacote terá sido completamente recebido no primeiro computador?

O tempo até o primeiro comutador é $t = 1,5.10^3/1,5.10^6 = 10^{-3}s$. O segundo pacote é completamente recebido no segundo comutador no instante $t' = 2.10^{-3}$.

\letra{c} Quanto tempo demorará para movimentar o arquivo da máquina de origem até a máquina de destino quando é usada segmentação de mensagem? Compare este resultado com sua resposta na parte `a' e comente.

O primeiro pacote demora $3.10^{-3}s$ para chegar à origem. Após ele chegam a cada $10^{-3}s$ um pacote. 
Assim o tempo total de transmissão é $3 . 10^{-3} + 4999 . 10^{-3} = 5002 . 10^{-3}s$.

\letra{d} Discuta as desvantagens da segmentação de mensagem.

No modelo de comutadores tipo store-and-forward é interessante ter mensagens pequenas, pois se mantém menos dados nos comutadores, eles precisam menos espaço e podem lidar com um fluxo mais heterogêneo de mensagens. Tem-se um atraso maior usando-se segmentação, apesar de ser bastante pequeno no exemplo.
Além disto, com os cabeçalhos, a quantidade de dados a serem transmitidos com segmentação é maior.

\chapter{Camada de Aplicação}

\section{Princípios das aplicações de rede}

Em toda aplicação de rede há sempre um \emph{cliente}, que envia a primeira mensagem, que seria uma solicitação.
O \emph{servidor} então responde à solicitação e há uma troca de mensagens entre eles.

Visto esta forma como se dão as aplicações de rede tem-se uma arquitetura mais empregada, que é a \emph{cliente-servidor}.
A principal característica do servidor é estar sempre on-line, aguardando as solicitações que sempre são iniciadas por clientes, que não necessariamente estão sempre on-line.

Na outra arquitetura possível, tem-se que os hosts da rede tem códigos idênticos, podendo agir como clientes e servidores, fornecendo e solicitando serviços.
Esta arquitetura é a \emph{peer-to-peer} pois os hosts são chamados de \emph{peers}.

As suas vantagens (tanto que elas estão sendo cada vez mais icentivadas) são a escalabilidade natural e 
o menor custo para manter toda uma infra-estrutura de servidores, fazendo com que ela seja considerada \emph{cost effective}.

O que limitação seu emprego são três fatores: a forma como as redes de acesso operam, sendo normalmente assimétricas as bandas up ou down;
o icentivo que tem que ser dado aos hosts (que já não são de uma empresas, pagos por elas e exclusivo para aquele uso) para que eles disponibilizem serviços; e a questão da segurança que advém de não se ter algo centralizado e tudo distribuído, sendo mais difícil saber em quem confiar.

Tendo estas arquiteturas, aborda-se como elas são implementadas.
As aplicações empregam \emph{sockets} que são estruturas mantidas pelos sistemas operacionais responsáveis por abstrair as especificidades da troca de mensagens entre os processos.

Estes sockets são, então, interfaces da camda de transporte, que fornecem alguns serviços para aplicações que a empreguem.
Dentre os serviços desejáveis tem-se a \emph{entrega confiável} (oferecida pelo TCP), que nem sempre deve ser necessária, 
tendo-se aplicações que podem lidar com perda de mensagens sem problemas (e usam UDP, no caso).
Outros serviços estão ligados ao \emph{throughput} e \emph{timing} que poderiam ser garantidos, o que não ocorre na Internet (por se usar comutação por pacotes, em prática).
Eles seriam interessante para uma série de aplicações, que devem lidar com as consequências de perdas da taxa fim-a-fim de transmissão (por uma série de motivos) e variações consideráveis no atraso de entregas.
Uma última garantia é a de segurança, fazendo com que só os aplicativos dos hosts comunicantes tenham conhecimento do conteúdo das mensagens.
Isto é implementado com a ``subcamada'' SSL, que encripta as mensagens antes dela ser enviada para o protocolo de transporte.

Na prática a Internet provê o TCP e o UDP. 
O primeiro é orientado a conexões, o que faz com que uma conexão tenha que ser mantida para haver a troca de dados, 
e tem-se garantias de entrega de mensagens, que são checadas por erros e são entregues em ordem.
O segundo não é orientado a conexão, ou seja, não se mantém estado e as mensagens são ``simplesmente'' enviadas.
A TCP tem também um controle de congestionamento, que permite que as mensagens não inundem a rede e faz com que se tenha certa ``justiça'' na distribuição dos recursos. 
O protocolo UDP ``permite'' que a rede seja inundada por um processo, o que pode ser interessante se quer-se desempenho e a possibilidade de usar todos os recursos necessários.

Outro serviço que é oferecido às aplicações na Internet é o de endereçamento.
Uma aplicação passa a ser unicamente identificada através do \emph{endereço IP} de seu hospedeiro e, como podem haver várias aplicações num mesmo host, a elas também estão associado um \emph{número de porta}.

\section{Protocolos de Aplicação}

Eles são padrões para a comunicação entre um par de processos, cada um agindo como cliente ou servidor em cada conexão.
Define-se uma semântica das mensagens, como se inicia, encerra as conexões e os campos onde serão explicitados o contexto da transmissão de dados.
Além disto, define-se o comportamento, ou seja, quando uma das partes envia uma mensagem e o seu conteúdo.

\section{DNS}

\section{Aplicações P2P}

\section{Exemplos de implementação com sockets}


\section{Questões}

\paragraph{Q6} Que informação é usada por um processo que está rodando em um hospedeiro para identificar um processo que está rodando em outro hospedeiro?

\paragraph{Q9} O que significa protocolo de apresentação?

\paragraph{Q12} Qual a diferença entre o HTTP persistente com paralelismo e HTTP persistente sem paralelismo? Qual dos dos é usado pelo HTTP/1.1?

\paragraph{Q15} Por que se diz que o FTP envia informações de controle 'fora da banda'?

\paragraph{Q16} Suponha que Alice envie uma mensagem a Bob por meio de uma conta de e-mail da Web (como o Hotmail), e que Bob acesse seu e-mail por seu servidor de correio usando POP3. Descreva como a mensagem vai do hospedeirao de Alice até o hospedeiro de Bob. Não se esqueça de relacionar a série de protocolos de camada de aplicação usados para movimentar a mensagem entre os dois hospedeiros.

\paragraph{Q19} É possível que o servidor Web e o servidor de correio de uma organização tenham exatamente o mesmo apelido para um nome de hospedeiro (por exemplo, \texttt{foo.com}? Qual seria o tipo de RR que contém o nome do hospedeiro do servidor de correio?

\paragraph{Q22} O servidor UDP descrito na seção 2.8 precisava de uma porta apenas, ao passo que o servidor TCP descrito na seção 2.7 precisava de duas portas. Por quê? Se o servidor TCP tivesse de suporta $n$ conexões simultâneas, cada uma de um hospedeiro cliente diferente, de quantas portas precisaria?

\paragraph{Q23} Para a aplicação cliente-servidor por TCP descrita na seção 2.7, por que o programa servidor deve ser executado antes do programa cliente? Para aplicação cliente-servidor por UDP descrita na seção 2.8, por que o programa cliente pode ser executado antes do programa servidor?

\subsection{Problemas}

6, 7, 9, 23

%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Camada de Transporte}

É responsável pelo transporte fim-a-fim, por prover à camada de transporte uma comunicação lógica entre dois processos.
A camada de rede que fica abaixo é responsável por realizar a comunicação entre dois processos.
A de transporte, empregando a de rede faz a comunicação entre dois processos.

\section{Multiplexação}

Como vários processos podem rodar em um mesmo host, um dos papéis da camada de transporte é fazer a multiplexação das mensagens no emitente (para que elas compartilhem dos canais lógicos de comunicação) e demultiplexar no destinatário, remontando as mensagens do processo emitente e entregando-as ao processo destino.

Para tal, os dados a serem trocados são inicialmente quebrados em \emph{segmentos} que são enviados, através do serviço de entrega oferecido pela camada de rede.
O cabeçalho adicionado por tal segmento tem que tem, em particular, um identificador dos processos que se comunicam.

Neste ponto, os dois principais protocolos de transporte são distintos.

No UDP não há estabelecimento de conexão, então qualquer processo é unicamente identificado na Web através do endereço do seu hospedeiro e de um \emph{número de porta} único que é escolhido pelo processo.
Assim, o segmento enviado contém estes dois dados que permitem a demultiplexação.
Outro campo empregado é a porta de origem, que é usada na camada de aplicação como porta de destino para responder à mensagem.

Já no TCP, esta identificação não basta.
Ao iniciar uma conexão, um processo envia uma mensagem para um host + porta do servidor.
Ele, então, cria um socket local com uma porta associada, que é enviada.

A questão é que o servidor tem um socket (host + porta) que aceita todas requisições de conexão, iniciando-as.
Assim, estando ela estabelecida, as mensagens que chegam para o mesmo host + porta sem o bit SYN ligado, são direcionadas ao processo que é identificado por, além do host+porta destino, do host+porta do emissor.
Isto porque um mesmo host pode fazer requisições na mesma porta do servidor (digamos, a 80 de um servidor Web) e, além disto, hosts diferentes podem escolher uma mesma porta local.

Assim, ao receber um segmento, o TCP vai analizar host + porta de partida e host + porta de destino para fazer a demultiplexação e entregara a mensagem ao processo correto.

\section{UDP}

Sua principal característica é não ter-se ideia de conexão. 
Assim, se adiciona às mensagens um cabeçalho simples e se envia elas ao destinatário.
As informações do cabeçalho são as essenciais: portas de saída e destino, tamanho do pacote (incluindo o cabeçalho) e um checksum.

As portas são usadas para entregar as mensagens ao processo certo e são o que fazem que não se use diretamente um pacote IP para realizar estas trocas de mensagens sem o overload de um protocolo de transporte.

O checksum é realizado sobre o segmento e também sobre parte do cabeçalho IP e consiste no complemento de 2 dos campos checados.
O destinatário então, checa se a soma de todos os campos, inclusive o checksum não contém bits 0.
Se houver problema, o UDP não lida com eles. Ele pode descartar a mensagem ou repassá-la ao processo junto a um warning.

As vantagens do UDP são relacionadas à sua simplicidade: cabeçalho curto (8 bytes), não gasta-se tempo com o estabelecimento de conexões e não se mantém estado, ou seja, não tem-se uma série de informações e buffers que devem ser mantidas associadas a cada socket, como no caso do TCP.
Outra grande vantagem é não ter-se as limitações de taxas de envio estipuladas pelo controle de congestionamento.
Por outro lado, a confiabilidade da comunicação e o tratamento de perdas deve ser feito pela aplicação.

\section{Transferência confiável de dados}

Esta seção constroi passo a passo um protocolo de comunicação confiável entre um emitente e um receptor.
Para tal, emprega-se uma série de mecanismos de checagem e controle que fazem com que a transmissão não-confiável (com perdas e/ou corrupção de pacotes) oferecida por uma camada inferior seja transformado num serviço confiável de entrega de dados, fornecido à camada superior.

Inicialmente cita-se o mecanismos chamados protocolos \emph{Automatica Repeat reQuest} ou \emph{ARQ}.
Eles contém, além das mensagens enviadas, mensagens de controle enviadas pelo receptor.
São o \emph{ACK}, que indica que a mensagem chegou corretamente e \emph{NACK}, que indica que a mensagem chegou incorretamente.

Estes protocolos baseiam-se no \emph{sender feedback} e tem que se basear em alguma forma de detecção de erros (corrupção).
A grande questão deles é de como se lidar com as retransmissões. 
A primeira alternativa é o \emph{stop-and-wait}, onde o emissor não envia novas mensagens enquanto não recebe um ACK e reenvia a mesma mensagem enquanto receber NACKs.

A limitação desta abordagem é desconsiderar a possibilidade dos pacotes de controles serem corrompidos.
Quando isto ocorre, o emissor não sabe se deve enviar uma nova mensagem ou reenviar a anterior, fazendo com que o receptor não saiba se uma mensagem está sendo reenviada ou se é nova.
Assim, este protocolo não é suficiente pois não lida com pacotes replicados.

Para fazer esta distinção, então, é necessário usar \emph{números de sequência} para os pacotes, que mesmo sendo binários, permitem saber se o pacote é novo ou duplicado.
Assim o emitente está no estado de envio do pacote $0$, passa para obtenção do ACK para o pacote $0$, passa para envio do pacote $1$, recepção do ACK para o pacote $1$ o que fecha o ciclo.

O interessante desta nova abordagem é que os NACKs passam a não ser mais necessário, pois um ACK para a sequência $0$ enquanto aguarda-se um ACK para $1$, ou seja, um \emph{ACK duplicado}, significa que o $1$ não foi recebido corretamente.
Assim, adicionando o número de sequência aos ACKs passa a se ter somente uma mensagem de controle.

Ao levar-se em consideração a possibilidade de perdas de pacotes, tem-se outro problema que é como o emissor sabe se o pacote chegou corretamente, já que tanto ele pode se perder assim como o ACK a ele associado.
Para tal é necessário que se tenha uma reação que independa do recebimento de uma mensagem (de dados ou controle), que seja temporal.
Assim, estipula-se um valor de tempo após o envio e se ele for atingido sem que não se tenha um feedback, o pacote é reenviado.

Isto é implementado com um \emph{countdown timer}, ativado sempre que se envia uma mensagem (de controle ou não) e desativado sempre que uma resposta chega.
Quando ele ``estoura'', um nova contagem de tempo é iniciado para o pacote duplicado enviado.

Este protocolo que lida com duplicatas através de número de sequências binários e com perdas com timeouts é conhecido por \emph{alternanting-bit-protocol}.
E ele é capaz de fazer um transporte confiável entre dois hosts, lidando com perdas e corrupção de mensagens.

A questão é que nesta abordagem \emph{stop-and-wait} o tempo efetivo de uso do canal é bem reduzido.
Isto porque somente uma mensagem pode estar trafegando por ele e nenhuma outra será enviada antes que o ACK correspondente seja recebido.
Para lidar com esta limitação, deve-se agir de forma a permitir que várias mensagens sejam enviadas antes que uma confirmação chegue.
Como deve-se manter a consistência da entrega, estes protocolos são considerados como uma forma de fazer um pipeline deste protocolo descrito anteriormente.

Há aqui duas vertentes, que lidam de forma diferente com os pacotes ainda não confirmados: o \emph{Go-back-n} e o \emph{selective repeat}.

\subsection{Go-back-N}

Neste protocolo o emissor pode enviar até $N$ mensagens antes de receber alguma confirmação de entrega.
Assim, define-se uma \emph{janela de transmissão} de tamanho $N$ que contém um intervalo de números de sequência que podem ser empregados para envio imediato de segmentos.
Esta janela é limitada inferiormente pela \emph{base}, que é o número sucessor ao último número de sequência confirmado e superiormente por $base + n$.
Dentro deste intervalo há um índice que é o próximo número de sequência empregável, usado como número do próximo segmento a ser enviado.

Para lidar com perdas e corrupções emprega-se igualmente um temporizador.
Caso ele estoure, reenvia-se todas as mensagens ainda não confirmadas, ou seja, com número de sequência no intervalo $[base, nextseqnum]$.
A cada ACK recebido do pacote $x$, a base passa a ser $x+1$ e o temporizador é reiniciado se houver ainda algum pacote a ser confirmado. 
Nota-se então que os ACKs são cumulativos, ou seja, representam que todos pacotes antes de $x$ foram recebidos corretamente.

Da parte do receptor, ele mantém somente uma variável com o número do próximo pacote esperado.
Chegando ele, gera-se um ACK para este valor, este número é incrementado e o pacote repassado para a aplicação (a recepção é em ordem).
Qualquer pacote com número maior ou menor é descartado, mas o último ACK é reenviado, porque ele poderia ter se perdido.

\subsection{Selective Repeat}

Esta abordagem vem da limitação do GBN de reenviar todos pacotes ainda não confirmados na falta de um ACK com número de sequência da base.
Em redes com muitas perdas isto pode gerar um tráfego grande de pacotes duplicados. 
Além disto, pacotes que chegam em ordem diferente do envio são sumariamente descartados (e terão de ser reenviados).

No SR tem-se a janela do emissor e do receptor com números mínimos $send_base$ e $rcv_base$ e mesmo tamanho $N$.
O receptor irá receber qualquer pacote do intervalo e confirmá-los independentemente.
Quando o menor pacote ou ACK esperados chegam, os $base$ são incrementados e o temporizador (que é individual por pacote enviado) é parado.

A entrega ainda se dá em ordem, mas a recepção não. 
Assim, pacotes duplicados e novos devem ser diferenciados, fazendo com que o tamanho da janela tenha que ser ao máximo metade do range de número de sequência.
Isto garante que cada número dentro da janela só possa ser um novo pacote ou uma cópia.

\subsection{Método empregado pelo TCP}

O TCP emprega um híbrido das técnicas mostradas, tentando obter o melhor delas.
Inicialmente, o número de sequência máximo é muito alto $2^{32} -1$ e o tamanho da janela é no máximo $2^{16} -1$, dependendo do controle de fluxo e congestionamento.
Porém, este número de sequência não é o número do segmento, mas sim a quantidade de bytes já enviados.

Assim, o envio se dá gerando o segmento com o próximo valor, incrementando o este valor com o tamanho dos dados e iniciando um temporizador, 
caso nenhum já esteja em curso.
A confirmação se dá por ACKs que contém o próximo número de sequência esperado, são cumulativos e levam ao reinício do temporizador, caso algum segmento não esteja ainda confirmado.

Com o estouro do temporizador, reenvia-se o último segmento ainda não confirmado e o timer é reiniciado.
O tempo de espera é obtido estatísticamente mas, no caso de estouro do timer, é configurado para o \emph{dobro} do valor anterior.

Do lado do receptor, mantém-se o próximo byte esperado e um buffer, que armazena pacotes com números de sequências maiores, mas dentro da janela.
Ao receber o pacote esperado o TCP não responde imediatamente, aguardando $500ms$ ou a chegada do próximo pacote (gerando um ACK cumulativo de dois segmentos).

Ao receber um pacote ``futuro'', um ACK é gerado para o pacote esperado.
No lador emissor, 3 ACKs duplicados para um segmento fazem com que ele seja reenviado, mesmo que o timer não tenha estourado, mecanismo conhecido como \emph{fast retransmit}.
Neste mesmo caso, se o receptor obtiver o byte esperado, ele e todos os bytes seguintes recebidos são entregues à aplicação e um ACK cumulativo é gerado.

Assim, evita-se o máximo retransmissões desnecessárias e possível congestionamento, pois geram-se ACKs cumulativos, só se retransmite o último pacote esperado e o timeout é duplicado em caso de atraso maior, desde que não haja novos ACKs ou novos envios.

\subsubsection{Estipulação dos timeouts}

O cálculo dos valores que deve ser empregado pelos temporizadores é feito através de estimativas, baseadas em medições de \emph{Round Times Trips} reais.
Elas são obtidas através de uma amostragem dos RTTs de alguns segmentos de uma conexão, dentre os que não são retransmitidos.

Tem-se o valor estimado para o RTT $EstimatedRTT = (1 - \alpha) . EstimatedRTT + \alpha . SampleRTT$, que é uma amostragem que exponencialmente decresce a influência de medidas antigas.
Além dele, usa-se a variação do RTT $VarRTT = (1 - \beta) . VarRTT + \beta . | EstimatedRTT - SampleRTT|$, com o mesmo formato.

Define-se o valor de timeout como sendo $EstimatedRTT + 4 . VarRTT$ e recomenda-se $\alpha = 0.125$ e $\beta = 0.25$.
Idealmente, o timeout é algo superior ao RTT (para evitar retransmissões desnecessárias) mas não tanto a ponto do tempo de resposta a perdas ser longo demais.


\subsection{Controle de fluxo TCP}

Não foi definido anteriormente o tamanho da janela TCP, pois ela é variável.
Esta abordagem é necessária para evitar que o emissor envie dados a uma taxa superior à taxa que o TCP processa, armazena os dados no buffer de entrada e, principalmente, à velocidade em que a aplicação lê (e, assim, libera espaço) do buffer de entrada.

No início da conexão, o receptor aloca o buffer de entrada com um tamanho que é o máximo tamanho da janela.
Este valor é repassado ao emissor, e é o tamanho incial de sua janela de envio, ou seja, o máximo número de bytes enviados e ainda não confirmados.
A cada confirmação o receptor atualiza o tamanho da janela do emissor, com o espaço disponível naquele momento no buffer.

Um problema desta abordagem é quando o buffer do receptor fica cheio, o que faz com que o emissor pare de enviar.
Neste caso, não havendo dados no sentido contrário, a janela de envio não será atualizada, mesmo havendo espaço no buffer do receptor.
Para contornar isto, o protocolo estabele que o emissor deve enviar periodicamente uma mensagem mínima para obter o tamanho de buffer disponível no receptor.

\section{TCP}

\subsection{Segmento TCP}

Um segmento TCP contém um cabeçalho, com uma série de informações úteis para o fluxo entre os processos.

Os primeiros campos são as portas de partida e destino (16 bits cada), usadas para multiplexação (o endereço está no pacote IP).

Depois tem-se valores usados na entrega de dados, ou seja o número de sequência do segmento e o número do ACK (32 bits cada um).
Como já dito, os segmentos são numerados com o primeiro byte do fluxo que eles carregam e os ACKs contém o próximo byte esperado do emissor.
Vale dizer que a conexão é \emph{full-duplex}, ou seja, os dois são emissores e receptores.
Desta forma, os ACKs costumam ir \emph{piggybacked} em pacotes com dados, o que é percebido por um bit que denota se o campo ACK é válido.

Outro campo de 16 bits é usado no controle de fluxo e contém o espaço disponível na janela de recepção.
Com 16 bits tem-se também o checksum do pacote, e há 3 bits para indicar ações de gerenciamento da conexão: \emph{SYN, FIN, RST}.

Há outros campos pouco usados atualmente.
O \emph{PSH} é um bit que diz se os dados devem ser enviados imeditamente à aplicação.
Outro bit é o \emph{URG} indica que o campo \emph{urgent data point} (com 16 bits) contém um ponteiro para certo dado que deve ser entregue na aplicação com urgência.

Finalmente, há um campo extra de opções que contém dados se o campo de tamanho de cabeçalho (4 bits) indicar mais que 5, 
que dá o tamanho mínimo do cabeçalho TCP de 20 bytes.

Além do cabeçalho tem-se os dados trocados entre os processos, cuja quantidade é variável e limitada pelo \emph{Maximum segment size} ou \emph{MSS}.
Ele é tal que, junto ao tamanho dos cabeçalhos TCP e IP não seja maior que o \emph{maximum transmission unit} ou \emph{MTU},
que o tamanho máximo de um \emph{frame} da camada de enlace de dados.

Há também protocolos que obtém o \emph{MTU} para o caminho mais provável entre os dois hosts, com o claro objetivo de evitar ao máximo que o segmento TCP seja fragmentado em algum nó intermediário da rede.

\subsection{Gerenciamento das conexões}

A abertura de uma conexão TCP é feita através de três passos, sendo conhecida como \emph{three-way handshape}:
\paragraph{1} O cliente envia um segmento TCP sem dados, com o bit \emph{SYN} ativado e com um número de sequência aleatório.
\paragraph{2} O servidor aloca os buffers e variáveis para a conexão e responde com outro pacote sem dados. 
Ele tem o bit \emph{SYN} ativado, um número de sequência e o número de sequência do cliente incrementado no campo ACK.
Ele é uma confirmação do estabelecimento da conexão, sendo chamado de \emph{SYNACK}.
\paragraph{3} Recebendo este pacote, o cliente aloca buffers e variáveis e responde ao servidor com o campo ACK adequado e, possivelmente, com dados.
\paragraph{} 

Nota-se que o passo 2 pode ser perigoso, pois inúmeras conexões podem ser abertas sem serem usadas.
Este ataque é conhecido como \emph{syn flood} e causa negação de serviço, pois várias conexões são meio-abertas para nada.

Há uma modificação no TCP que evita tal ataque, empregando \emph{syn cookies}.
Nela, o servidor não cria a estrutura e responde com um número de sequência especial, calculado a partir dos endereços e uma chave secreta.
Se um cliente legítimo retornar um ACK, o servidor checa se o número de sequência é correto e inicializa a estrutura, que não é comprometida.

O fechamento de uma conexão TCP é feito com quatro passos:
\paragraph{1} Uma das partes envia um segmento com o bit \emph{FIN} ativado
\paragraph{2} A outra parte envia um ACK, desaloca recursos e envia um \emph{FIN}
\paragraph{3} A solicitante recebe o \emph{FIN} e envia um ACK
\paragraph{4} O solicitante aguarda por um tempo (estado \emph{TIMEWAIT}) antes de desalocar os recursos completamente

\paragraph{} Além destas possibilidades, há o uso do bit \emph{RST}. 
Ele é empregado para responder segmentos para alguma porta de destino não alocada.

\section{Controle de Congestionamento}

Protocolos como o TCP fazem com que conexões entre hosts através de links não confiáveis sejam oferecidas às aplicações como conexões lógicas entre hosts, com garantias de entrega confiável.
Assim, os mecanismos já citados da camada de transporte visam lidar principalmente com a perda de pacotes.
Esta perda está associada principalmente ao descarte de pacotes que é realizado pelos roteadores do core da rede, quando a taxa de chegada de dados é superior à taxa em que tais pacotes podem ser transmitidos pelo enlace de saída.

Assim, os mecanismos de entrega confiável empregam retransmissões para tratar de sintomas do congestionamento da rede, 
mas não tratam as causas de tais perdas.
E tais causas normalmente estão associadas a muitos processos e/ou hosts que enviam pacotes à uma taxa mais alta que a suportada pelos enlaces.

As consequências destas inundações são diretamente percebidas pelas aplicações, que notam uma degradação, um subuso da rede.
Isto porque as camadas inferiores não lidam com os pacotes como dados, como algo que tem semântica definida, ou seja, não distinguem um envio de uma retransmissão.

É interessante notar que mesmo em cenários irreais, o congestionamento ocorreria. 
Ou seja, mesmo com buffers infinitos, havendo duas conexões que compartilham um mesmo enlace com taxa $R$ haverá perda de eficiência se estas duas conexões estiverem usando, conjuntamente, boa parte ou toda a largura da rede.

Neste caso, isto se deve a um dos mecanismos usados para inferir perdas, que é o RTT acima do esperado.
Quando um roteador começa a operar com taxas de entrada que se aproximem à sua capacidade de saída o tempo de resposta cresce exponencialmente.

Isto faz com os temporizadores estourem e pacotes (que serão entregues, pois não há perdas) sejam desnecessariamente retransmitidos, fazendo com que parte do esforço de transmissão seja desperdiçado com pacotes duplicados, que serão descartados no destino.
Assim, neste cenário irrealístico, a taxa efetiva de transmissão das várias conexões será inferior ao esperado.

Como na realidade os buffers são finitos, tem-se perdas quando a fila de saída do roteador está cheia.
Assim alguns pacotes chegam e outros não chegam ao destino, mas, visto o grande delay, a camada de transporte não saberá distinguí-los, reenviando todos.

Isto pode ser ainda mais complicado se se supõe que há vários roteadores no caminho do pacote, todos sobrecarregados.
Neste caso, mesmo que um pacote não seja descartado em algum deles, há grande probabilidade dele ser descartado em outros intermediários,
fazendo com que o throupughput efetivo tenda a zero quando as taxas de envio somadas forem próximas à capacidade da rede.

Assim, o congestionamento deve ser evitado ou tratado, sob risco de se ter uma estagnação na comunicação entre processos da rede.
Mostra-se a seguir as principais abordagens empregáveis.

\subsection{Network-assisted}

Nesta abordagem a camada de rede (ou seja, os roteadores) provêm feedback explícito sobre o tráfego de rede.
Isto pode ser feito através da sinalização (ativação de algum bit no cabeçalho de rede) nos pacotes que trafegam em um ambiente congestionado, 
através de mensagens especiais enviadas pelos roteadores aos emissores e/ou 
através de mensagens especiais trocadas pelos hosts finais e com campos alterados durante o seu percurso na rede.

Um exemplo desta abordagem é o mecanismo \emph{Avaliable Bit Rate} (\emph{ABR}) empregado nas redes \emph{ATM}.
Nele, além de pacotes de dados há pacotes especiais \emph{Resource Management Cells} ou \emph{RM cells} que trafegam junto aos pacotes comuns mas não contém carga útil.

Há três abordagens de sinalização de congestionamento.
Na primeira os roteadores modificam o bit \emph{EFCI} de pacotes comuns que trafeguem em ambientes congestionados.
Ao receber um pacote todo host checa este bit e achando-o ativado, envia uma célula RM com bit \emph{Congestion Indication} ativado para o emissor.

Além destes avisos de emergência, tem-se as células RM que trafegam na rede, interpostas entre conjunto de pacotes comuns.
Elas possuem um campo de \emph{taxa explícita}, que pode ser decrementado pelos roteadores, fazendo com que os hosts finais saibam a taxa mais alta possível na rede.
Além deles tem-se o CI já citado e o \emph{No Increase} que significa que tem-se risco de congestionamento se a taxa for incrementada.

Com base nos dados obtidos do tráfego destas células RM pela rede, calcula-se a taxa máxima de envio nos hosts finais e tem-se um mecanismo eficaz para evitar e tratar do congestionamento.

\subsection{End-to-end}

Quando a camada de rede não provê feedback explícito do tráfego, a camada de transporte deve inferir o estado da rede através do comportamento do tráfego fim-a-fim.
Este é o caso do protocolo IP, o que faz com que o TCP, empregando as taxas de perda e a quantidade de ACKs duplicados, tente inferir se há congestionamento.

Identificando possíveis situações do gênero, o TCP deve reduzir sua taxa de envio de pacotes, o que é feito limitando o tamanho da janela de transmissão,
que já é limitada também pelo controle de fluxo.
Ao mesmo tempo, estando a rede livre, espera-se que o limite de tamanho da janela não seja mais estabelecido pelo congestionamento (que não há),
mas sim pelo controle de fluxo.

Assim, o TCP age da seguinte forma.
Quando um segmento é perdido (ou seja, há um timeout ou três ACKs duplicados) a taxa de transmissão deve cair.
Quando a confirmação de um segmento é recebida, a rede entregou o pacote e a taxa de transmissão pode ser aumentada.
Mas até quando ela pode ser aumentada? No caso do TCP, ela é aumentada até que haja perdas, ou seja, até que se tenha um alerta de possível congestionamento, e, neste caso, ela deve ser reduzida (normalmente de forma brusca).

\section{Controle de Congestionamento TCP}

Consiste em manter a taxa de envio em dois estágios distintos:

\paragraph{Partida lenta} A conexão é normalmente iniciada com janela mínima de 1 \emph{MSS}. A partir daí, a cada ACK recebido, a janela é incrementada. Assim, a cada RTT, a janela é duplicada. Este crescimento se dá exponencialmente até que se chegue a um \emph{threshold} pré-determinado.

\paragraph{Congestion Avoidance} A partir do \emph{threshold}, o crescimento da janela passa a ser linear com os RTTs. Assim, a cada RTT incrementa-se o tamanho da janela em 1 \emph{MSS}

\paragraph{Fast Recovery} Recebendo-se um \emph{ACK}, vai-se para o \emph{congestion avoidance}. Esta fase é recomendada e o TCP Tahoe não implementa ela.

\paragraph{Timeout} Em qualquer destes estágios, se há um timeout para algum pacote, a janela é reduzida a \emph{1 MSS} e volta-se à \emph{partida lenta}. 
O \emph{threshold} é configurado para a metade do valor atual da janela.

\paragraph{Três ACKs duplicados} O \emph{threshold} é configurado para a metade do valor atual da janela. No TCP Reno a janela é reduzida para o novo valor do \emph{threshold} e vai-se para o \emph{fast recovery}. No Tahoe, o comportamento é igual a de um timeout.

Por suas características este mecanismo tem-se uma adição incremental e um descrescimento multiplicativo (\emph{AIMD}), 
levando a um comportamento serrilhado.
Ignorando a partida lenta, o crescimento é linear até passar do limite e fazer com que algum segmento seja perdido e receba-se o terceiro ACK.
Aí a janela é dividida pela metade (o que pode ser conservador) e continua-se crescendo-a linearmente.

\subsection{Justiça}

Além de evitar que a rede seja inundada, o controle de congestionamento teria como objetivo também distribuir igualitariamente dentre as várias conexões a banda disponível.
Isto não ocorre exatamente com o TCP, visto que RTTs maiores tem crescimento mais lento na janela de transmissão e assim são prejudicados.

Porém, o mecanismo per si tende a levar conexões com RTTs próximos a, após certo tempo, dividirem a banda igualitariamente.
É interessante notar que a justiça é entre conexões e não entre processos. Se um mesmo processo fizer múltiplas conexões, ele ocupará maior parte da banda, provavelmente.

Vale notar que o UDP não tem controle de banda e, assim, pode ``tomar'' toda banda de conexões TCP que, vendo indícios de congestionamento vão reduzir a taxa de transmissão.
É bom notar que não ter restrições de banda é uma vantagem do UDP, mas que, normalmente, a maior parte do tráfego UDP é bloqueado nos firewalls.

\subsection{Adendos ao TCP}

\paragraph{TCP Vegas} Calcula-se a diferença \emph{vazão esperada - (tamanho da janela/minimo RTT)}. 
Se ela for superior a um $\beta$, há um acréscimo linear da janela.
Se for inferior a um $\alpha$, há um decréscimo linear da janela.
O congestionamento é inferido a partir do RTT, não pelas perdas.
Assim, as perdas são mais raras, o que faz com que a reação (o fast recovery) para ACK duplicados seja imediato (não a cada três).
O crescimento da janela é exponencial a cada RTT não incrementada a cada ACK.
O resultado é um tamanho de janela mais estável e convergente.

\paragraph{TCP Sack} Opção negociável onde o receptor pode comunicar quais segmentos faltam (confirmação seletiva). 
A retransmissão é condicionada a não ter-se em trânsito mais segmentos que o tamanho da janela.
Assim, evita-se retransmissões desnecessárias.

\paragraph{TCP Fack} Modifica-se o TCP Sack de forma de manter o número de segmentos em trânsito e não confirmados menores que a janela.

\paragraph{TCP New Reno} Modificação do TCP Reno em que no \emph{fast recovery} cada ACK duplicado aumenta em 1 MSS o tamanho da janela, 
sendo encerrado com um ACK válido.

\paragraph{Stream Control Transmission Protocol (SCTP)} 
Alternativa ao TCP com suporte a novas funcionalidades.
As principais são o \emph{multi-streaming} e \emph{multi-homing}, que permitem que uma mesma conexão contenha múltiplos fluxos e que os dados usem endereços e caminhos diferentes.
Os dados são divididos em vários chunks que são reunidos no destino e entregue as aplicações. 
Além do mais, os chunks podem ter tanto dados como sinalizações e o estabelecimento da conexão evita o \emph{syn flood}, com 4 passos de comunicação.
A entrega pode ser ordenada ou não, sendo indicada para dados multimídia também.

\paragraph{Opções para o UDP} Há o \emph{UDP Lite} e o \emph{Datagram Congestion Control Protocol} que trazem o controle de congestionamento para o UDP, 
possibilitando usar a semântica simples do UDP para conexões na Internet (para multimidia, por exemplo).

\subsection{TCP para redes com alto produto banda-atraso}

O controle de congestionamento se dá através de perdas e o crescimento da janela por RTT.
Se o RTT é alto a janela cresce muito lentamente (mesmo havendo largura de banda).
Se a banda é bem larga (redes de alta velocidade) a janela máxima é atingida após muito tempo, com taxas irrealisticamente baixas de erros.

As abordagens para redes de alta velocidade é obter crescimento mais rápido e implementar técnicas de crescimento baseado em atraso ou mistas.

Quando tem-se muito atraso, como nos enlaces por satélite, emprega-se soluções genéricas que empregam estimativas de banda passante e possibilitam um crescimento mais rápido da janela, não dependente completamente do RTT.
Outra opção é segmentar a conexão, empregando um protocolo comum nos enlaces normais e um protocolo específico nos enlaces via satélite.

\subsection{TCP para redes sem fio}

Os problemas estão associados a alta taxa de erros inerente do meio, que serão tratadas como falso indício de congestionamento.
Outra questão são as perdas de sinal e os handoffs, que são desconexões e caem no mesmo problema.

Uma das linhas de solução é a segmentação da conexão entre a rede sem fio e rede normal.
A escolha a ser feita é a capacidade de tratamento de erro + suporte a handoff contra o tamanho dos buffers que as estações bases terão de ter (para mascarar as perdas).

Outra linha mantém a conexão fim-a-fim e tenta discriminar o erro do enlace de rede das perdas por congestionamento.
Pode-se usar outros modelos de detecção de erros (como o jitter), congelar os relógios quando há erro local ou permissões para crescimento mais rápido da janela, mesmo com os erros.
Pode-se também mascarar as perdas do ambiente também.
Outra linha viola a autonomia fim-a-fim e tem-se notificações entre emissor e receptores.

Para o ambiente \emph{ad-hoc} o grande problema é a perda de rotas e conexões.
Neste caso não pode-se abstrair os problemas da camada de rede tratando-os como erros.
Assim, viola-se a independência das camadas se se quiser usar TCP.

\section{Questões}

\paragraph{Q1} Fonte é $y$ e o destino é $x$.

\paragraph{Q2} Esta escolha depende da aplicação. 
Se ela tem mecanismos para lidar com (ou não muda nada para ela) a perda e o desordenamento de pacotes, é interessante. 
Se ela quer velocidade e usar completamente a banda, ou seja enviar mensagens a qualquer hora e a qualquer taxa e ter pacotes com cabeçalhos menores (e assim, transmissão mais rápida).
Além disto se a rede entre os hosts é rápida e com muitos poucos erros.

\paragraph{Q3} Sim, é possível. 
Será necessário implementar os mecanismos de entrega confiável (possivelmente em ordem) na camada de aplicação.
Para tal é necessário identificar a perda de pacotes (com ACKs e timers ou através de sequência semântica da aplicação) e numerá-los para que a entrega seja em ordem.

\paragraph{Q4} a. falso (alguns ACKs são obrigatórios)
b. falso (o controle de congestionamento e de fluxo o fazem)
c. verdade (é o princípio do controle de fluxo)
d. falso (o número depende do tamanho do pacote)
e. verdade
f. falso (depende do do DevRTT, pois o SampleRTT tem peso 1/8)
g. falso (os números de ACK e sequência são independentes no mesmo segmento)

\paragraph{Q5} a. tem X - 90 bytes de dados
b. 90, pois o ACK é dado para o último número de sequência recebido em ordem

\paragraph{Q6} Enviará um seg com seq=44 e ack=80 e receberá duas respostas com seq=82 e ack=...

\paragraph{Q7} Metade, ou seja, $R/2$.

\paragraph{Q8} Falso. Quando o timeout do emitente expira o tamanho da janela cai para 1 MSS e o threshold vai para a metade do tamanho da janela quando houve o timeout.

\paragraph{P10} O protocolo do bit alternante pode falhar se houver reordenamento de mensagens.

\paragraph{P11} No segundo caso seria interessante, pois o envio poderia ser imediato (sem aguardo de confirmações) e somente as raras falhas seriam reportadas e os segmentos equivalentes reenviados. Neste caso a recepção poderia ser fora de ordem e seria necessário reordená-la.
No primeiro caso não seria interessante, pois as perdas não seriam conhecidas pois há poucos dados enviados e o gap pode não ser reconhecidos (ou vai demorar).

\paragraph{P12} Tem-se que o $\frac{nL/R}{RTT + L/R}$ é o uso da rede. Como tem-se $0.027\%$ precisa-se $n = 90/0.027 = 3333.3333 < 3334$.

\paragraph{P19a} Quando recebe-se um ACK para um determinado número de sequência a base é incrementada e este valor fica fora da janela. Se o número de sequência máximo não for suficiente, pode-se receber uma ACK ``perdido'' e que já foi reenviado muito tempo depois. Neste caso, ele será descartado.

\paragraph{P19b} Pelo mesmo motivo.
\paragraph{P19c e d} Verdade. Em ambos os casos se envia uma mensagem e se descartam outras mensagens fora de ordem até que um ACK volte.

\paragraph{P20} a. o número de sequência é associado ao tamanho. Assim, com $2^{32}$ valores é possível mandar este número de bytes.

b. O atraso de transmissão será de $n.L/R$. O número de segmentos é de $2^{32}/1460 = 2941758$. O tamanho de cada é $1460 + 66 = 1526$ e o tempo de transmissão será de $2941759 . 1526 .8 / 10^6$.

\paragraph{P25} Pois a entrega é fora de ordem. Assim, supondo o ACK para o segmento $x$ é bem possível que o segmento $x+1$ seja recebido após o $x + 2$ e por aí vai.

\paragraph{P27} Entre as rodadas 1 e 6 e 23 e 26 teve-se a duplicação da janela a cada rodada (RTT). Está-se na partida lenta.
Entre as rodadas 6 e 15, entre 17 e 22 está-se no controle de congestionamento, visto fato que a janela cresce em 1 MSS.
Na rodada 15 houveram 3 ACKs duplicados e a janela caiu pela metade.
Na rodada 22 houve um timeout estourado.
O valor do threshold inicial era de 32 MSS, que é o valor da janela que causou o incío do congestion avoidance. 
Na rodada 16 ele caiu para 21, pois houve detecção de congestionamento e ele era 42 antes (na rodada 15).
Na rodada 22 a janela era 26 e caiu para 13 após a detecção do congestionamento.
Rodadas e segmentos: 1,1; 2,3; 3,7; 4, 15; 5, 31; 6, 63; 7, 96. Assim, o segmento 70 foi enviado na rodada 7.
Na rodada 26 a janela tem tamanho 8 e threshold de 13. Perdendo-se um pacote nesta rodada teria-se janela e threshold de tamanho 4.

\paragraph{P30} O controle de fluxo TCP limita o tamanho da janela do emissor a partir do espaço livre no buffer do receptor. Como este é suficiente para manter todo o fluxo este limite não existiria.
Mesmo sendo a taxa de chegada de dados ao TCP do emitente superior à capacidade do canal em 10 vezes, o controle de congestionamento não vai reduzir o tamanho da janela, pois não há perdas ou duplicação de ACKs.
Como o buffer de saída é $1\%$ do tamanho do arquivo, se ele tiver tamanho menor que $R$ bits ele também passará a ser uma limitação.
Assim, a janela máxima possível (que seria de $R$ bytes) nunca seria atingível.

\paragraph{P33} A vantagem é ter uma janela inicial bastante grande e poder enviar muitos pacotes logo no primeiro timeout. A questão são as outras conexões. Se a rede tiver tomado um estado de grande tráfego, o envio de uma quantidade imensa de pacotes irá aumentar o problema. Até porque vários deles não irão chegar. Neste caso, empregando o Reno terá uma perda e a janela cairá para apenas um pacote.

\subsection{Problema 34}

Nela faz-se o cálculo da latência para envio de dados continuamente passados pela aplicação para o outro host usando uma conexão TCP.
São dados o $RTT$, o tamanho $O$ do arquivo, o MSS $S$, o tamanho $W$ da janela de transmissão e taxa de transmissão $R$.

Os valores dados são o $RTT = 100ms$, $O = 100kbytes$ e $S = 536 bytes$.
Pergunta-se a latência mínima e o $W$ mínimo necessário, com taxas $R_1 = 28kbps$, $R_2 = 100kbps$, $R_3 = 1Mbps$ e $R_4 = 10 Mbps$.


%%%%%%%%%%%%%%5

\chapter{Camada de Rede}

Esta é uma das camadas mais importantes e mais desafiadoras da Internet.
O seu papel é realizar a entrega de pacotes entre dois hosts quaisquer, 
através do \emph{roteamento} e \emph{forwarding} dos pacotes nos nós intermediários da rede.
Assim, esta é uma das camadas presentes no core da rede.

O \emph{forwarding} é a operação realizada pelos \emph{packet switches}, recebendo pacotes em uma interface (enviadas por um nó através de um enlace) 
e enviando-os por outra interface (outro enlace, levando-os a outro nó da rede).
Esta decisão é feita através de uma busca em uma \emph{forwarding table} pela interface de entrada + bits obtidos do cabeçalho do pacote.
Se tal cabeçalho é o da camada de rede tem-se um \emph{roteador}.

Já o \emph{roteamento} é um cálculo de um caminho entre dois hosts quaisquer na rede, ou seja, a definição de uma série de nós intermediários que devem ser passados por um pacote para alcançar o destino.
Em cada um destes nós, ocorre o \emph{forwarding} segundo uma tabela que é atualizada através de um protocolo de roteamento.

Em alguns modelos de rede (que não incluem o modelo da Internet) esta camada também é responsável por realizar a \emph{conection setup}.
Ela consiste na reserva de recursos nos nós intermediários da rede antes que a comunicação propriamente dita (a troca de dados) ocorra.

\section{Modelos de serviço}

É possível que a camada de rede forneça \emph{garantias de entrega}: pacotes cheguem e possivelmente dentro de um período de tempo definido.
Além disto, é possível que se tenha \emph{entrega ordenada} e \emph{garantias de segurança} como encriptação, integridade de dados e autenticação da origem.
É possível estabelecer outras formas mais flexíveis de qualidade de transmissão, como \emph{jitter máximo} e \emph{taxa de transmissão mínima}.

A Internet usa um modelo de \emph{best-effort} que não garante nada disto.
Nas redes \emph{ATM} tem-se várias garantias, divididades em duas classes:

\paragraph{Constant Bit Rate} Interessante para tráfego telefônico e multimídia. Há reserva de recursos e de uma taxa fixa de transmissão. 
Há limites aceitáveis de jitter e de perda ou atraso.
Antes das transmissões as conexões devem ser iniciadas e tais valores são negociados com a rede ATM.
Não há congestionamento.

\paragraph{Avaliable Bit Rate} Modo mais flexível, onde se garante uma taxa mínima de transmissão 
e caso haja recursos disponíveis, a transmissão pode usar maior banda.
A entrega é ordenada, podem haver perdas e o controle de congestionamento é realizado pela camada de rede, com células especiais de controle.

\subsection{Circuitos Virtuais}

É o modelo de transporte na camada de rede que oferece uma conexão entre dois hosts na rede.
Assim, antes do envio de dados há uma etapa de inicialização.
Dado o endereço do destinatário calcula-se o caminho (uma série de enlaces e roteadores) que são informados da conexão através de mensagens de sinalização.

Cada roteador recebe uma mensagem do tipo a partir de um enlace $a$ e com um identificador $CV_i$ os associa a um outro enlace $b$ e outro $CV_{i+1}$.
Esta quádrula $a, CV_i, b, CV_{i+1}$ é armazenada na tabela de forwarding e mantida lá até que receba-se outra mensagem que encerre tal conexão.
Assim, há uma transmissão entre os dois hosts interessados (ida e volta) que permite a configuração da rota (através de cada forward)
e uma possível negociação e alocação dos recursos necessários.

Durante o envio dos dados o mesmo caminho é sempre empregado. 
Cada nó do percurso preenche o campo $CV$ especificado na tabela e o coloca na interface indicada, 
sendo que os nós intermediários adicionalmente leem o cabeçalho do pacote para realizar a busca na tabela.

\subsection{Comutação de Pacotes}

Este é o modelo empregado pela Internet.
Para enviar uma mensagem um host preenche um endereço (único na rede) no cabeçalho do pacote e o põem na rede.
Não há o conceito de conexão, ou seja, o caminho não é previamente conhecido e nenhum recurso da rede é alocado.

Cada roteador extrai o endereço e busca na tabela pela interface para onde ele deve redirecionado.
Como os endereços podem ser grandes, emprega-se uma tabela de prefixos e procura-se pela \emph{longest prefix matching rule}.

Nota-se que nenhum estado é mantido nos roteadores relacionado a determinada comunicação.
Esta tabela, porém, é modificada por algoritmos de roteamento, mas com muito menos frequência que a tabela dos CVs.
Isto também faz com que pacotes de um mesmo fluxo de dados tomem destinos diferentes, não havendo garantia de ordenação na entrega.

\paragraph{}

A distinção deste modelos advém de suas origens.
Os CVs se originam na telefonia, cuja característica é ter-se um núcleo da rede complexo e uma borda ``burra''.
Já na Internet tem-se interesse que o núcleo da rede seja simples (a ponto de possibilitar que várias tecnologias sejam usadas) e a complexidade se encontra na borda.

\section{Funcionamento dos routeadores}

\section{Questões}

Questões: 2, 8, 19, 21, 22, 24, 32, 35, 36
Problemas: 7, 21, 23


\bibliographystyle{plain}
\bibliography{../references}
\end{document}
